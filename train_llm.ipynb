{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from mention import Mention, decode_mentions, encode_bio, decode_bio, split_text\n",
    "from score import ScoringCounts, score_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "hf_auth = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'dataset/train_annotation.jsonl'\n",
    "dev_data_path = 'dataset/dev_annotation.jsonl'\n",
    "test_data_path = 'dataset/test_annotation.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "lora_rank = 16\n",
    "lora_alpha = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quatization parameters\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='bfloat16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "        output_dir='Llama_3.2_1B_Zelda_NER_3',\n",
    "        max_seq_length=256,\n",
    "        packing=True,\n",
    "        bf16=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=25,\n",
    "        learning_rate=learning_rate,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs = {'use_reentrant': True},\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_format(instance):\n",
    "    prompt = (\n",
    "        f'Task: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\n'\n",
    "        f'Input: {instance[\"text\"]}\\n'\n",
    "        f'Output: {instance[\"candidates\"][0]}\\n'\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_format_eval(instance):\n",
    "    prompt = (\n",
    "        f'Task: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\n'\n",
    "        f'Input: {instance[\"text\"]}\\n'\n",
    "        f'Output: '\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Start Processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    train_dataset = Dataset.from_list(list(map(json.loads, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=False,\n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    token=hf_auth\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OG\\AppData\\Local\\Temp\\ipykernel_12540\\758618563.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Applying formatting function to train dataset: 100%|██████████| 1471/1471 [00:00<00:00, 37711.00 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 1471/1471 [00:00<00:00, 54465.23 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 1471/1471 [00:00<00:00, 52519.40 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1471/1471 [00:00<00:00, 5945.93 examples/s]\n",
      "Packing train dataset: 100%|██████████| 1471/1471 [00:00<00:00, 13017.76 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=prompt_format,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 04:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nlp\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6817decb-6136ffdb309d6db91dcea585;7833bb14-27b9-4323-8d9d-5d68e22c4695)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlp\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlp\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6817df57-2fa5a8bb40747f7758091739;7062fe00-fedf-4b42-a663-581d1f349487)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlp\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=48, training_loss=2.1935609181722007, metrics={'train_runtime': 297.0188, 'train_samples_per_second': 5.141, 'train_steps_per_second': 0.162, 'total_flos': 2286483491782656.0, 'train_loss': 2.1935609181722007})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config,\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "model_dev = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"Llama_3.2_1B_Zelda_NER_3/checkpoint-48\",\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_data_path, 'r', encoding='utf-8') as f:\n",
    "    dev_dataset = Dataset.from_list(list(map(json.loads, f)))\n",
    "dev_text = [instance['text'] for instance in dev_dataset]\n",
    "dev_prompts = [prompt_format_eval(instance) for instance in dev_dataset]\n",
    "dev_labels = [instance['candidates'][0] for instance in dev_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = dev_prompts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Task: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\nInput: B-b-because! Just beyond Goron City, they're rainin' down from the sky!\\nOutput: \", \"Task: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\nInput: The Fang and Bone only opens up at nighttime. Apparently it's at Skull Lake, but I don't know where that is.\\nOutput: \"]\n"
     ]
    }
   ],
   "source": [
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2 [00:00<?, ?batch/s]c:\\Users\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 2/2 [00:11<00:00,  5.99s/batch]\n"
     ]
    }
   ],
   "source": [
    "model_dev.eval()\n",
    "\n",
    "responses = []\n",
    "for i in tqdm(range(0, len(test_text), batch_size), desc=\"Processing\", unit=\"batch\"):\n",
    "    batch = test_text[i:i + batch_size]\n",
    "    inputs = tokenizer(\n",
    "        batch,\n",
    "        return_tensors=\"pt\",  # Ensure output is in tensor format\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model_dev.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id, # Explicitly set pad token id\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    '''\n",
    "    input_lengths = inputs.input_ids.shape[1]\n",
    "    response_ids = generated_ids[:, input_lengths:]\n",
    "    generated_text = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
    "    '''\n",
    "\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    for text in generated_text:\n",
    "        responses.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Task: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\nInput: B-b-because! Just beyond Goron City, they're rainin' down from the sky!\\nOutput: 1. B 2. b 3. b 4. b 5. b 6. b 7. b 8. b 9. b 10. b\\nTask: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\nInput: B-b-because! Just beyond Goron City, they're rainin' down from the sky!\\nOutput: 1. B 2. b 3. b 4. b 5. b 6. b 7. b 8. b 9. b 10. b\\nTask: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\nInput: B-b-because! Just beyond Goron City, they're rainin' down from the sky!\\nOutput: 1. B 2. b 3. b 4. b 5. b 6. b 7. b 8. b 9. b 10. b\\nTask: You are an expert Named Entity Recognition system specialized in The Legend\", \"Task: You are an expert Named Entity Recognition system specialized in The Legend of Zelda: breath of the wild. Your task is to identify and tag entities in the provided text.\\nInput: The Fang and Bone only opens up at nighttime. Apparently it's at Skull Lake, but I don't know where that is.\\nOutput: 1. Fang 2. Bone 3. Night 4. Open 5. Night 6. Lake 7. Skull 8. Lake 9. Skull 10. Lake 11. Skull 12. Lake 13. Skull 14. Lake 15. Skull 16. Lake 17. Skull 18. Lake 19. Skull 20. Lake 21. Skull 22. Lake 23. Skull 24. Lake 25. Skull 26. Lake 27. Skull 28. Lake 29. Skull 30. Lake 31. Skull 32. Lake 33. Skull 34. Lake 35. Skull 36. Lake 37. Skull 38. Lake 39. Skull 40. Lake 41. Skull 42. Lake 43. Skull 44. Lake 45. Skull 46. Lake 47. Skull 48. Lake 49. Skull 50. Lake 51. Skull 52. Lake 53. Skull 54. Lake 55. Skull 56. Lake 57. Skull 58. Lake 59. Skull 60. Lake 61. Skull 62. Lake 63. Skull 64. Lake \"]\n"
     ]
    }
   ],
   "source": [
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_clean_text = [split_text(item) for item in dev_text]\n",
    "target_mentions = []\n",
    "for item, clean_text in zip(dev_labels, dev_clean_text):\n",
    "    mention = decode_mentions(item, clean_text)\n",
    "    target_mentions.append(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mentions = []\n",
    "for item, clean_text in zip(responses, dev_clean_text):\n",
    "    mention = decode_mentions(item, clean_text) # may need to be changed since the roduce text may be different\n",
    "    predict_mentions.append(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(dev_dataset)):\n",
    "    result = score_mentions(target_mentions[i], predict_mentions[i])\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true_positive = [score_counts[0] for score_counts in results]\n",
    "total_false_positive = [score_counts[1] for score_counts in results]\n",
    "total_false_negative = [score_counts[2] for score_counts in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = sum(total_true_positive) / (sum(total_true_positive) + sum(total_false_positive)) if (sum(total_true_positive) + sum(total_false_positive)) > 0 else 0\n",
    "recall = sum(total_true_positive) / (sum(total_true_positive) + sum(total_false_negative)) if (sum(total_true_positive) + sum(total_false_negative)) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
