{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f32bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import tqdm as tqdm\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "from mention import Mention, decode_bio\n",
    "from score import ScoringCounts, score_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa9f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd867d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = '<unk>'\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94f79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(vocab, embedding_dim):\n",
    "    glove_model = api.load(f\"glove-wiki-gigaword-{embedding_dim}\")\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for index, word in enumerate(vocab):\n",
    "        if word in glove_model:\n",
    "            embedding_matrix[index] = glove_model[word]\n",
    "        else:\n",
    "            embedding_matrix[index] = np.random.normal(scale=0.1, size=(embedding_dim,))\n",
    "    embedding_matrix[0] = np.zeros(embedding_dim)\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640e8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoderPreTrain(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embed_dim, hidden_dim, num_tags, num_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "        embedding_matrix = load_glove_embeddings(vocab, embed_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), padding_idx=0, freeze=True)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.linear = nn.Linear(hidden_dim*2, num_tags)\n",
    "        else:\n",
    "            self.linear = nn.Linear(hidden_dim, num_tags)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # (1, src_len, embed_dim)\n",
    "        input_embeds = self.embedding(input_ids)\n",
    "\n",
    "        # (1, src_len, hidden_dim)\n",
    "        lstm_hidden, _ = self.lstm(input_embeds)\n",
    "\n",
    "        # (1, src_len, num_tags) -> (n, src_len, num_tags)\n",
    "        emission = self.linear(lstm_hidden)\n",
    "        return emission # (n, src_len, num_tags)\n",
    "\n",
    "    def decode(self, input_ids):\n",
    "        emission = self(input_ids) # (n, src_len, num_tags)\n",
    "        probs = nn.functional.softmax(emission, dim=1) # (n, src_len, num_tags)\n",
    "        preds = torch.argmax(probs, dim=1) # (n, src_len)\n",
    "        return preds\n",
    "\n",
    "    def nll_loss(self, input_ids, labels):\n",
    "        # (src_len, num_tags)\n",
    "        emission = self(input_ids)\n",
    "        preds = nn.functional.log_softmax(emission, dim=2) # (n, src_len, num_tags)\n",
    "        N, d, c = preds.shape\n",
    "        loss = nn.functional.nll_loss(preds.reshape(N*d, c), labels.reshape(N*d), ignore_index=0)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1bce201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCRF(nn.Module):\n",
    "    def __init__(self, src_vocabs, tgt_vocabs, embed_dim, hidden_dim, num_layers, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_vocabs = src_vocabs\n",
    "        self.tgt_vocabs = tgt_vocabs\n",
    "        self.num_tags = len(tgt_vocabs)\n",
    "\n",
    "        self.lstm = LSTMEncoderPreTrain(\n",
    "          vocab=src_vocabs,\n",
    "          embed_dim=embed_dim,\n",
    "          hidden_dim=hidden_dim,\n",
    "          num_tags=len(tgt_vocabs),\n",
    "          num_layers=num_layers,\n",
    "          bidirectional=bidirectional\n",
    "        )\n",
    "        self.transitions = nn.Parameter(torch.rand(self.num_tags, self.num_tags))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.lstm(input_ids), self.create_mask(input_ids) # (n, src_len, num_tags)\n",
    "    \n",
    "    def create_mask(self, input_ids):\n",
    "        mask = (input_ids != 0).float().to(device) \n",
    "        return mask\n",
    "\n",
    "    def forward_alg(self, emission, mask):\n",
    "        batch_size, seq_len, _ = emission.size()\n",
    "        alpha = torch.full((batch_size, self.num_tags), -1000., device=device)\n",
    "        alpha[:, 0] = 0\n",
    "        for w in range(seq_len):\n",
    "            alpha_w = []\n",
    "            mask_w = mask[:, w].view(batch_size, 1).expand(batch_size, self.num_tags)\n",
    "            for next_tag in range(self.num_tags):\n",
    "                emit_score = emission[:, w, next_tag].view(batch_size, 1).expand(batch_size, self.num_tags)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1).expand(batch_size, self.num_tags)\n",
    "                next_tag_var = alpha + trans_score + emit_score\n",
    "                alpha_w.append(torch.logsumexp(next_tag_var, dim=1).view(batch_size, 1))\n",
    "            new_alpha = torch.cat(alpha_w, dim=1)\n",
    "            alpha = torch.where(mask_w.bool(), new_alpha, alpha) # (n, num_tags-1)\n",
    "        alpha = torch.logsumexp(alpha, dim=1) # (n)\n",
    "        return alpha\n",
    "\n",
    "    def score(self, emission, labels, mask):\n",
    "        batch_size, seq_len, _ = emission.size()\n",
    "        score = torch.zeros(batch_size, device=device)\n",
    "        for i in range(seq_len):\n",
    "            mask_i = mask[:, i]\n",
    "            if i == 0:\n",
    "                score += emission[range(batch_size), i, labels[:, i]] * mask_i\n",
    "            else:\n",
    "                score += (\n",
    "                    self.transitions[labels[:, i], labels[:, i-1]] +  # Transition\n",
    "                    emission[range(batch_size), i, labels[:, i]]       # Emission\n",
    "                ) * mask_i\n",
    "    \n",
    "        return score # (n)\n",
    "\n",
    "    def decode(self, input_ids):\n",
    "        is_single_instance = False\n",
    "        if input_ids.dim() == 1:\n",
    "            is_single_instance = True\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        emission, mask = self(input_ids)\n",
    "    \n",
    "        batch_size, seq_len, _ = emission.size()\n",
    "        \n",
    "        backpointers = []\n",
    "\n",
    "        forward_vvars = torch.full((batch_size, self.num_tags), -1000., device=device) # (n, num_tags)\n",
    "        forward_vvars[:, 0] = 0\n",
    "\n",
    "        for w in range(seq_len):\n",
    "            bptrs_t = []\n",
    "            viterbivars_t = []\n",
    "            for next_tag in range(self.num_tags):\n",
    "                next_tag_var = forward_vvars + self.transitions[next_tag].view(1, -1).expand(batch_size, self.num_tags) # (n, num_tags)\n",
    "                best_tag_id = torch.argmax(next_tag_var, dim=1) # (n)\n",
    "                bptrs_t.append(best_tag_id) # (n)\n",
    "                viterbivars_t.append(next_tag_var[range(batch_size), best_tag_id].view(batch_size, 1))\n",
    "            forward_vvars = (torch.cat(viterbivars_t, dim=1) + emission[:, w]).view(batch_size, -1) * mask[:, w].view(batch_size, 1) + forward_vvars * (1 - mask[:, w].view(batch_size, 1))\n",
    "            backpointers.append(bptrs_t)\n",
    "            #backpointers.append(torch.stack(bptrs_t, dim=1))\n",
    "\n",
    "        best_tag_id = torch.argmax(forward_vvars, dim=1)\n",
    "        path_score = forward_vvars[range(batch_size), best_tag_id]\n",
    "\n",
    "        best_paths = []\n",
    "        for j in range(batch_size):\n",
    "            seq_end = int(mask[j].sum().item())\n",
    "            best_path = [best_tag_id[j].item()]\n",
    "            for bptrs_t in reversed(backpointers[:seq_end]):\n",
    "                best_tag_id = bptrs_t[best_path[-1]][j]\n",
    "                best_path.append(best_tag_id.item())\n",
    "\n",
    "            best_path.reverse()\n",
    "            best_paths.append(best_path)\n",
    "        \n",
    "        if is_single_instance:\n",
    "            return best_paths[0]\n",
    "        else:\n",
    "            return best_paths\n",
    "        #return path_score, best_paths\n",
    "\n",
    "    def nll_loss(self, input_ids, labels):\n",
    "        # (src_len, num_tags)\n",
    "        emission, mask = self(input_ids)\n",
    "        forward = self.forward_alg(emission, mask) # (n)\n",
    "        score = self.score(emission, labels, mask) # (n)\n",
    "        return (forward - score).mean() # (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b519b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_vocabs(training_data):\n",
    "    src_vocabs_list, tgt_vocabs_list = set(), set()\n",
    "    for item in training_data:\n",
    "        for word in item['text']:\n",
    "            src_vocabs_list.add(word)\n",
    "        for label in item['label']:\n",
    "            tgt_vocabs_list.add(label)\n",
    "\n",
    "    src_vocabs_list = sorted(src_vocabs_list)\n",
    "    tgt_vocabs_list = sorted(tgt_vocabs_list)\n",
    "\n",
    "    # words (tokens) vocab\n",
    "    src_vocabs = {x: i for i, x in enumerate([PAD, UNK] + src_vocabs_list)}\n",
    "\n",
    "    # NER labels vocab (and its inverse)\n",
    "    tgt_vocabs, tgt_vocabs_inv = dict(), dict() \n",
    "    for i, x in enumerate([PAD] + tgt_vocabs_list):\n",
    "        tgt_vocabs[x] = i\n",
    "        tgt_vocabs_inv[i] = x\n",
    "\n",
    "    return src_vocabs, tgt_vocabs, tgt_vocabs_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43eb3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, src_vocabs, tgt_vocabs):\n",
    "    pad_num = 30\n",
    "\n",
    "    out = []\n",
    "    for item in data:\n",
    "        cur_data = []\n",
    "        word_list = item['text']\n",
    "        labels = item['label']\n",
    "\n",
    "        tgt = [tgt_vocabs[x] for x in labels]\n",
    "        if len(tgt) > pad_num:\n",
    "            tgt = tgt[:pad_num]\n",
    "        else:\n",
    "            tgt += [0] * (pad_num - len(tgt))\n",
    "        # (1, src_len+1)\n",
    "        tgt_tensor = torch.tensor(tgt, dtype=torch.int64)\n",
    "\n",
    "\n",
    "        # tensorized source data (sentence tokens)\n",
    "        src = [src_vocabs.get(x, src_vocabs[UNK]) for x in word_list]\n",
    "        if len(src) > pad_num:\n",
    "                src = src[:pad_num]\n",
    "        else:\n",
    "            src += [0] * (pad_num - len(src))\n",
    "        # (src_len)\n",
    "        src_tensor = torch.tensor(src, dtype=torch.int64)\n",
    "\n",
    "        cur_data.insert(0, src_tensor)\n",
    "        cur_data.insert(1, tgt_tensor)\n",
    "        out.append(cur_data)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b595ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(reference_labels, predict_labels, tgt_vocabs_inv):\n",
    "    reference_labels = [x.item() if torch.is_tensor(x) else x for x in reference_labels]\n",
    "    predict_labels = [x.item() if torch.is_tensor(x) else x for x in predict_labels]\n",
    "    # convert to string labels\n",
    "    reference_labels = [tgt_vocabs_inv[x] for x in reference_labels]\n",
    "    predict_labels = [tgt_vocabs_inv[x] for x in predict_labels]\n",
    "\n",
    "    # remove padding labels\n",
    "    reference_labels = [x for x in reference_labels if x != PAD]\n",
    "    predict_labels = [x for x in predict_labels if x != PAD]\n",
    "\n",
    "    return reference_labels, predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6c61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_matrix(reference_labels, predict_labels):\n",
    "    reference = decode_bio(reference_labels)\n",
    "    predict = decode_bio(predict_labels)\n",
    "    matrix = score_mentions(reference, predict)\n",
    "    return matrix, reference, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e367597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tagger, eval_dataset, tgt_vocabs_inv, device, epoch):\n",
    "    outputs_list = []\n",
    "    TP = FP = FN = 0\n",
    "    with torch.no_grad():\n",
    "        for eval_data in tqdm.tqdm(eval_dataset):\n",
    "            # dev has labels but test doesn't\n",
    "            input_ids = eval_data[0].to(device)\n",
    "            labels = eval_data[1] if eval_data[1] != [] else None\n",
    "\n",
    "            preds = tagger.decode(input_ids) # (n, src_len)\n",
    "\n",
    "            if torch.is_tensor(preds):\n",
    "                if preds.ndim == 2:\n",
    "                        preds = preds[0]\n",
    "                preds = preds.tolist()\n",
    "\n",
    "            reference_labels, predict_labels = process_output(labels, preds, tgt_vocabs_inv)\n",
    "            matrix, reference, predict = calculation_matrix(reference_labels, predict_labels)\n",
    "            outputs_list.append({'reference': reference, 'predict': predict})\n",
    "            TP += matrix[0]\n",
    "            FP += matrix[1]\n",
    "            FN += matrix[2]\n",
    "\n",
    "    # calculate precision, recall and f1 score\n",
    "    print(f'TP: {TP}, FP: {FP}, FN: {FN}')\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    with open(f'CRF_Outputs/test_outputs_CRF_2_{epoch}.json', 'w') as f:\n",
    "        json.dump(outputs_list, f)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a93e8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "NUM_HIDDEN = 256\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "SEED = 1334\n",
    "EVAL_EVERY = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5583a",
   "metadata": {},
   "source": [
    "***Start Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72fea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'dataset/train_BIO.jsonl'\n",
    "dev_data_path = 'dataset/dev_BIO.jsonl'\n",
    "test_data_path = 'dataset/test_BIO.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9324f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = list(map(json.loads, open(train_data_path)))\n",
    "dev_instances = list(map(json.loads, open(dev_data_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d5d1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nsrc_vocabs, tgt_vocabs, tgt_vocabs_inv = collect_vocabs(instances)\\nwith open('src_vocabs.json', 'w') as src_file:\\n    json.dump(src_vocabs, src_file, indent=4)\\n\\nwith open('tgt_vocabs.json', 'w') as tgt_file:\\n    json.dump(tgt_vocabs, tgt_file, indent=4)\\n\\nwith open('tgt_vocabs_inv.json', 'w') as tgt_file:\\n    json.dump(tgt_vocabs_inv, tgt_file, indent=4)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('BiLSTM_vocab/src_vocabs.json', 'r') as src_file:\n",
    "    src_vocabs = {k: int(v) for k, v in json.load(src_file).items()}\n",
    "\n",
    "with open('BiLSTM_vocab/tgt_vocabs.json', 'r') as tgt_file:\n",
    "    tgt_vocabs = {k: int(v) for k, v in json.load(tgt_file).items()}\n",
    "\n",
    "with open('BiLSTM_vocab/tgt_vocabs_inv.json', 'r') as tgt_file:\n",
    "    tgt_vocabs_inv = {int(k): v for k, v in json.load(tgt_file).items()}\n",
    "'''\n",
    "\n",
    "src_vocabs, tgt_vocabs, tgt_vocabs_inv = collect_vocabs(instances)\n",
    "with open('src_vocabs.json', 'w') as src_file:\n",
    "    json.dump(src_vocabs, src_file, indent=4)\n",
    "\n",
    "with open('tgt_vocabs.json', 'w') as tgt_file:\n",
    "    json.dump(tgt_vocabs, tgt_file, indent=4)\n",
    "\n",
    "with open('tgt_vocabs_inv.json', 'w') as tgt_file:\n",
    "    json.dump(tgt_vocabs_inv, tgt_file, indent=4)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5927fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = vectorize(instances, src_vocabs, tgt_vocabs)\n",
    "dev_dataset = vectorize(dev_instances, src_vocabs, tgt_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54355810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  39,   99, 3094, 2356, 2559, 3167, 3331, 3785,  813, 2867, 3770,  815,\n",
      "        1029,   25,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]), tensor([9, 3, 7, 9, 9, 9, 9, 9, 9, 9, 9, 3, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eea47085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMEncoderPreTrain(\n",
       "  (embedding): Embedding(3831, 300, padding_idx=0)\n",
       "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = LSTMEncoderPreTrain(\n",
    "        vocab=src_vocabs,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=NUM_HIDDEN,\n",
    "        num_tags=len(tgt_vocabs),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL\n",
    "    )\n",
    "tagger.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76a89cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMCRF(\n",
       "  (lstm): LSTMEncoderPreTrain(\n",
       "    (embedding): Embedding(3831, 300, padding_idx=0)\n",
       "    (lstm): LSTM(300, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = LSTMCRF(\n",
    "        src_vocabs=src_vocabs,\n",
    "        tgt_vocabs=tgt_vocabs,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=NUM_HIDDEN,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL\n",
    "    )\n",
    "tagger.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3361db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(tagger.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c32b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(training_dataset) % BATCH_SIZE == 0:\n",
    "    batchCount = len(training_dataset) // BATCH_SIZE\n",
    "else:\n",
    "    batchCount = len(training_dataset) // BATCH_SIZE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c427c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10]: 100%|██████████| 92/92 [00:10<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Loss: 1151.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/10]: 100%|██████████| 92/92 [00:10<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/10] Loss: 642.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/10]: 100%|██████████| 92/92 [00:09<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/10] Loss: 460.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10]: 100%|██████████| 92/92 [00:10<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10] Loss: 337.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5/10]: 100%|██████████| 92/92 [00:10<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to CRF_Outputs/outputs_CRF_2_5.pt\n",
      "Begin evaluation on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:05<00:00, 36.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 173, FP: 205, FN: 89\n",
      "[Epoch 5/10] Loss: 263.07 | Dev F1: 0.5406249999999999%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6/10]: 100%|██████████| 92/92 [00:10<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/10] Loss: 194.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7/10]: 100%|██████████| 92/92 [00:10<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/10] Loss: 123.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8/10]: 100%|██████████| 92/92 [00:10<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/10] Loss: 82.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10]: 100%|██████████| 92/92 [00:10<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10] Loss: 39.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10/10]: 100%|██████████| 92/92 [00:10<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to CRF_Outputs/outputs_CRF_2_10.pt\n",
      "Begin evaluation on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:04<00:00, 37.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 164, FP: 222, FN: 98\n",
      "[Epoch 10/10] Loss: 17.31 | Dev F1: 0.5061728395061728%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    random.shuffle(training_dataset)\n",
    "    for i in tqdm.tqdm(range(batchCount), desc=f'[Epoch {epoch}/{NUM_EPOCHS}]'):\n",
    "        if i == batchCount - 1:  # Last mini-batch\n",
    "            mini_batch = training_dataset[i * BATCH_SIZE:]\n",
    "        else:\n",
    "            mini_batch = training_dataset[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "        mini_input_ids = torch.stack([instance[0] for instance in mini_batch])\n",
    "        mini_labels = torch.stack([instance[1] for instance in mini_batch])\n",
    "        mini_input_ids, mini_labels = mini_input_ids.to(device), mini_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = tagger.nll_loss(mini_input_ids, mini_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # display info at the end of epoch\n",
    "    log = f'[Epoch {epoch}/{NUM_EPOCHS}] Loss: {epoch_loss:.2f}'\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        # Save the model at the end of each epoch\n",
    "        save_path = f\"CRF_Outputs/outputs_CRF_2_{epoch}.pt\"\n",
    "        torch.save(tagger.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "        print(\"Begin evaluation on dev set\")\n",
    "        dev_f1 = evaluate(tagger, dev_dataset, tgt_vocabs_inv, device=device, epoch=epoch)\n",
    "        log = f'{log} | Dev F1: {dev_f1}%'\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dd731",
   "metadata": {},
   "source": [
    "***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd181c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OG\\AppData\\Local\\Temp\\ipykernel_10356\\3871593582.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tagger.load_state_dict(torch.load(checkpoint_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMCRF(\n",
       "  (lstm): LSTMEncoderPreTrain(\n",
       "    (embedding): Embedding(3831, 300, padding_idx=0)\n",
       "    (lstm): LSTM(300, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tagger = LSTMEncoderPreTrain(\n",
    "        vocab=src_vocabs,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=NUM_HIDDEN,\n",
    "        num_tags=len(tgt_vocabs),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL\n",
    "    )\n",
    "tagger.to(device)\n",
    "'''\n",
    "\n",
    "tagger = LSTMCRF(\n",
    "        src_vocabs=src_vocabs,\n",
    "        tgt_vocabs=tgt_vocabs,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=NUM_HIDDEN,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL\n",
    "    )\n",
    "tagger.to(device)\n",
    "\n",
    "# Load the saved model weights\n",
    "checkpoint_path = \"CRF_Outputs/outputs_CRF_2_5_model.pt\"  # Replace with your .pt file path\n",
    "tagger.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2aeb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = list(map(json.loads, open(test_data_path)))\n",
    "test_dataset = vectorize(test_instances, src_vocabs, tgt_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae5c869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [00:05<00:00, 36.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 159, FP: 204, FN: 89\n",
      "Test F1: 0.5204582651391162%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_f1 = evaluate(tagger, test_dataset, tgt_vocabs_inv, device=device, epoch=1)\n",
    "print(f\"Test F1: {test_f1}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
